{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd28ea0-94f7-4099-ad58-6273ff3c9da5",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f57cc-0f50-4615-8772-c540288b6ea1",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for both classification and regression problems. It belongs to the ensemble learning methods, where multiple weak learners (often decision trees) are combined to create a strong predictive model. In the context of regression, it is specifically referred to as Gradient Boosting Regression.\n",
    "\n",
    "Here's a brief overview of how Gradient Boosting Regression works:\n",
    "\n",
    "Weak Learners: The algorithm starts with an initial weak learner (usually a shallow decision tree). This model makes predictions, but its accuracy is typically limited.\n",
    "\n",
    "Residuals: The algorithm then focuses on the errors made by the initial model. It calculates the difference between the actual values and the predicted values, known as residuals.\n",
    "\n",
    "Weighted Learning: A new weak learner is trained to fit the residuals. The idea is to give more weight to the instances where the initial model performed poorly. This process is repeated iteratively, with each new learner focusing on the mistakes of the combined model so far.\n",
    "\n",
    "Combination: The predictions from all the weak learners are combined to produce the final model. The combination is often done by summing up the predictions, with each learner's prediction weighted according to its contribution to reducing the overall errors.\n",
    "\n",
    "Gradient Descent: The term \"gradient\" in Gradient Boosting refers to the use of gradient descent optimization to minimize the loss function. The algorithm adjusts the parameters of the weak learners in the direction that minimizes the loss.\n",
    "\n",
    "The iterative nature of Gradient Boosting allows it to continuously improve and refine its predictions by learning from the mistakes of the previous models. This makes it a powerful and effective algorithm, often achieving high predictive accuracy.\n",
    "\n",
    "Popular implementations of Gradient Boosting Regression include the Gradient Boosting Machines (GBM), XGBoost, LightGBM, and CatBoost. These implementations often incorporate optimizations and parallel processing to enhance performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc93be2-69af-4fa4-b417-12bc5439b918",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9581912-59f0-4512-9358-beb4d7e9f148",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21c40b14-745a-4aac-ad52-fe7b3afd0567",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d756ef8f-d32d-4632-89e0-2b1d2121be5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eb70c0e-fbd4-49da-9645-dfa1e640beb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92abe5be-996a-4495-add0-834ce70a9e38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38a36292-58cc-476b-b66c-f1be194c595f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1859061c-13a8-430d-a8f0-b2d4b09124c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656d454f-51a2-4c1b-a4b7-c1bc892705c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fc1f98e-7cee-4224-94f4-7f37cf94326f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f66aea60-dd0e-4f4a-b382-e4ca14c22821",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf0b0dd5-71df-40b5-88c1-f29708f1b101",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7ff6ed-937d-4774-b7e3-9effa271ac80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "54b7036e-684f-44ab-a84f-b95f64759448",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
